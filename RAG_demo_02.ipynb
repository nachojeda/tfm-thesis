{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG app TFM v2\n",
    "In this use case it is shown how to extract information from a PDF file through LLM queries with RAG (Retrieval Augmented Generation) technology. For this use case is necessary the use of a vector database (in this case FAISS), embeddings and OpenAI model calls. To show the final result, the model is embedded on a Gradio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "load_dotenv(\"apis.env\")\n",
    "# hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\anaconda3\\envs\\TFM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# CSS Template \n",
    "theme = gr.themes.Base(\n",
    "    primary_hue=\"rose\",\n",
    ").set(\n",
    "    body_background_fill='*neutral_50',\n",
    "    body_text_color='*neutral_500',\n",
    "    body_text_weight='300',\n",
    "    background_fill_primary='*neutral_50',\n",
    "    background_fill_secondary='*primary_50',\n",
    "    border_color_primary='*primary_400',\n",
    "    color_accent_soft='*primary_300',\n",
    "    link_text_color='*primary_300',\n",
    "    link_text_color_active='*neutral_300',\n",
    "    link_text_color_hover='*primary_100',\n",
    "    link_text_color_visited='*neutral_400',\n",
    "    code_background_fill='*primary_200',\n",
    "    button_secondary_background_fill='*neutral_100',\n",
    "    button_secondary_border_color='*neutral_900',\n",
    "    button_secondary_text_color='*primary_400',\n",
    "    button_cancel_background_fill='*primary_600',\n",
    "    button_cancel_background_fill_hover='*primary_700',\n",
    "    button_cancel_text_color='*neutral_50',\n",
    "    slider_color='*primary_500'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7890\n",
      "Running on local URL:  http://127.0.0.1:7905\n",
      "Running on public URL: https://d9455b25233aa1380d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d9455b25233aa1380d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "def extract_sentence(dictionary):\n",
    "    # Extract the value associated with the key 'result'\n",
    "    sentence = dictionary.get('result', '')\n",
    "    return sentence\n",
    "\n",
    "def model_hyperparameters(temperature, max_tokens, db):#\n",
    "    llm = OpenAI(\n",
    "        model_name=\"gpt-3.5-turbo-instruct\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        streaming=True\n",
    "        )\n",
    "    chain = RetrievalQA.from_llm(llm=llm, retriever=db.as_retriever())\n",
    "    return chain\n",
    "\n",
    "def respond(message, output_label, temperature=0.7, max_tokens=32):\n",
    "    db = pdf_vectorized_loader(output_label)\n",
    "    prompt = message\n",
    "    chain = model_hyperparameters(temperature, max_tokens, db)\n",
    "    completion = chain(prompt, return_only_outputs=True)\n",
    "    return extract_sentence(completion)\n",
    "\n",
    "def pdf_vectorized_loader(pdf_file):\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load_and_split()\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    db = FAISS.from_documents(pages, embeddings)\n",
    "    return db\n",
    "\n",
    "def process_file(uploaded_file):\n",
    "    if uploaded_file is not None:\n",
    "        filename = uploaded_file.name\n",
    "        return filename\n",
    "    return \"No file uploaded.\"\n",
    "\n",
    "with gr.Blocks(theme=theme) as demo: \n",
    "    gr.Markdown(\"# TFM RAG App\")\n",
    "    gr.Markdown(\"With this RAG app, you can generate augmented responses from a selected PDF file.\")\n",
    "\n",
    "    file_input = gr.File(file_types=[\".pdf\"])\n",
    "    output_label = gr.Textbox(visible=False)\n",
    "\n",
    "    file_input.change(process_file, inputs=file_input, outputs=output_label)\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask a question\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1.0, value=0.2, step=0.1, info=\"Regulates the creativity of the answers\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", value=64, maximum=256, minimum=8, step=1, info=\"Regulates the length of the answers\")\n",
    "    completion = gr.Textbox(label=\"Response\")\n",
    "    btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    clear = gr.ClearButton(components=[msg, completion], value=\"Clear console\", variant=\"stop\")\n",
    "    \n",
    "    btn.click(respond, inputs=[msg, output_label, temperature, max_tokens], outputs=[completion])\n",
    "    msg.submit(respond, inputs=[msg, output_label, temperature, max_tokens], outputs=[completion])\n",
    "\n",
    "    gr.Markdown(\"Created by Ignacio Ojeda Sánchez (www.linkedin.com/in/ignacio-ojeda-sánchez-610924225)\", header_links=True)\n",
    "\n",
    "gr.close_all()\n",
    "demo.queue().launch(share=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7890\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evalutation using Llama Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_questions = []\n",
    "with open('eval_questions.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Remove newline character and convert to integer\n",
    "        item = line.strip()\n",
    "        print(item)\n",
    "        eval_questions.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "tru = Tru()\n",
    "\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"./eBook-How-to-Build-a-Career-in-AI.pdf\"\n",
    "prompt = \"What is AI?\"\n",
    "# res = respond(message, doc)\n",
    "temperature = 0.2\n",
    "max_tokens = 128\n",
    "\n",
    "db = pdf_vectorized_loader(doc)\n",
    "chain = model_hyperparameters(temperature, max_tokens, db)\n",
    "completion = chain(prompt, return_only_outputs=True)\n",
    "res = extract_sentence(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AI stands for artificial intelligence. It is a rapidly growing field that involves using computer systems to perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. AI has the potential to transform and improve various areas of human life.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Context Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Context Relevance, input response will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input source will be set to __record__.app.query.rets.source_nodes[:].node.text .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# from utils import get_prebuilt_trulens_recorder\n",
    "from trulens_eval import (\n",
    "    Feedback,\n",
    "    TruLlama,\n",
    "    OpenAI\n",
    ")\n",
    "from trulens_eval.feedback import Groundedness\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "qa_relevance = (\n",
    "    Feedback(openai.relevance_with_cot_reasons, name=\"Answer Relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "\n",
    "qs_relevance = (\n",
    "    Feedback(openai.relevance_with_cot_reasons, name = \"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(TruLlama.select_source_nodes().node.text)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "#grounded = Groundedness(groundedness_provider=openai, summarize_provider=openai)\n",
    "grounded = Groundedness(groundedness_provider=openai)\n",
    "\n",
    "groundedness = (\n",
    "    Feedback(grounded.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "        .on(TruLlama.select_source_nodes().node.text)\n",
    "        .on_output()\n",
    "        .aggregate(grounded.grounded_statements_aggregator)\n",
    ")\n",
    "\n",
    "feedbacks = [qa_relevance, qs_relevance, groundedness]\n",
    "\n",
    "def get_prebuilt_trulens_recorder(query_engine, app_id):\n",
    "    tru_recorder = TruLlama(\n",
    "        query_engine,\n",
    "        app_id=app_id,\n",
    "        feedbacks=feedbacks\n",
    "        )\n",
    "    return tru_recorder\n",
    "\n",
    "tru_recorder = get_prebuilt_trulens_recorder(chain,\n",
    "                                             app_id=\"TFM RAG App\")\n",
    "\n",
    "\n",
    "# embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\", embed_batch_size=10)\n",
    "# llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     llm=llm, embed_model=embed_model\n",
    "# )\n",
    "# index = VectorStoreIndex.from_documents([document],\n",
    "#                                         service_context=service_context)\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\n",
    "#     \"What are the keys to building a career in AI?\"\n",
    "# )\n",
    "# print(str(response))\n",
    "\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# def extract_sentence(dictionary):\n",
    "#     # Extract the value associated with the key 'result'\n",
    "#     sentence = dictionary.get('result', '')\n",
    "#     return sentence\n",
    "\n",
    "# def model_hyperparameters(temperature, max_tokens, db):#\n",
    "#     llm = OpenAI(\n",
    "#         model_name=\"gpt-3.5-turbo-instruct\",\n",
    "#         temperature=temperature,\n",
    "#         max_tokens=max_tokens,\n",
    "#         streaming=True\n",
    "#         )\n",
    "#     chain = RetrievalQA.from_llm(llm=llm, retriever=db.as_retriever())\n",
    "#     return chain\n",
    "\n",
    "# def respond(message, output_label, temperature=0.7, max_tokens=32):\n",
    "#     db = pdf_vectorized_loader(output_label)\n",
    "#     prompt = message\n",
    "#     chain = model_hyperparameters(temperature, max_tokens, db)\n",
    "#     completion = chain(prompt, return_only_outputs=True)\n",
    "#     return extract_sentence(completion)\n",
    "\n",
    "# def pdf_vectorized_loader(pdf_file):\n",
    "    # loader = PyPDFLoader(pdf_file)\n",
    "    # pages = loader.load_and_split()\n",
    "    # embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "    # db = FAISS.from_documents(pages, embeddings)\n",
    "    # return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_recorder as recording:\n",
    "    for question in eval_questions:\n",
    "        response = extract_sentence(chain(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>app_json</th>\n",
       "      <th>type</th>\n",
       "      <th>record_id</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tags</th>\n",
       "      <th>record_json</th>\n",
       "      <th>cost_json</th>\n",
       "      <th>perf_json</th>\n",
       "      <th>ts</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [app_id, app_json, type, record_id, input, output, tags, record_json, cost_json, perf_json, ts, latency, total_tokens, total_cost]\n",
       "Index: []"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
