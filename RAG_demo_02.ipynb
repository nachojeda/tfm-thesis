{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG app TFM v2\n",
    "In this use case it is shown how to extract information from a PDF file through LLM queries with RAG (Retrieval Augmented Generation) technology. For this use case is necessary the use of a vector database (in this case FAISS), embeddings and OpenAI model calls. To show the final result, the model is embedded on a Gradio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "load_dotenv(\"apis.env\")\n",
    "# hf_api_key = os.environ['HF_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nacho\\anaconda3\\envs\\TFM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# CSS Template \n",
    "theme = gr.themes.Base(\n",
    "    primary_hue=\"rose\",\n",
    ").set(\n",
    "    body_background_fill='*neutral_50',\n",
    "    body_text_color='*neutral_500',\n",
    "    body_text_weight='300',\n",
    "    background_fill_primary='*neutral_50',\n",
    "    background_fill_secondary='*primary_50',\n",
    "    border_color_primary='*primary_400',\n",
    "    color_accent_soft='*primary_300',\n",
    "    link_text_color='*primary_300',\n",
    "    link_text_color_active='*neutral_300',\n",
    "    link_text_color_hover='*primary_100',\n",
    "    link_text_color_visited='*neutral_400',\n",
    "    code_background_fill='*primary_200',\n",
    "    button_secondary_background_fill='*neutral_100',\n",
    "    button_secondary_border_color='*neutral_900',\n",
    "    button_secondary_text_color='*primary_400',\n",
    "    button_cancel_background_fill='*primary_600',\n",
    "    button_cancel_background_fill_hover='*primary_700',\n",
    "    button_cancel_text_color='*neutral_50',\n",
    "    slider_color='*primary_500'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7890\n",
      "Running on local URL:  http://127.0.0.1:7894\n",
      "Running on public URL: https://d547fea7505b25d9f5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d547fea7505b25d9f5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "def extract_sentence(dictionary):\n",
    "    # Extract the value associated with the key 'result'\n",
    "    sentence = dictionary.get('result', '')\n",
    "    return sentence\n",
    "\n",
    "def model_hyperparameters(temperature, max_tokens, db):#\n",
    "    llm = OpenAI(\n",
    "        model_name=\"gpt-3.5-turbo-instruct\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        streaming=True\n",
    "        )\n",
    "    chain = RetrievalQA.from_llm(llm=llm, retriever=db.as_retriever())\n",
    "    return chain\n",
    "\n",
    "def respond(message, output_label, temperature=0.7, max_tokens=32):\n",
    "    db = pdf_vectorized_loader(output_label)\n",
    "    prompt = message\n",
    "    chain = model_hyperparameters(temperature, max_tokens, db)\n",
    "    completion = chain(prompt, return_only_outputs=True)\n",
    "    return extract_sentence(completion)\n",
    "\n",
    "def pdf_vectorized_loader(pdf_file):\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    pages = loader.load_and_split()\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(pages, embeddings)\n",
    "    return db\n",
    "\n",
    "def process_file(uploaded_file):\n",
    "    if uploaded_file is not None:\n",
    "        filename = uploaded_file.name\n",
    "        return filename\n",
    "    return \"No file uploaded.\"\n",
    "\n",
    "with gr.Blocks(theme=theme) as demo: \n",
    "    gr.Markdown(\"# TFM RAG App\")\n",
    "    gr.Markdown(\"With this RAG app, you can generate augmented responses from a selected PDF file.\")\n",
    "\n",
    "    file_input = gr.File(file_types=[\".pdf\"])\n",
    "    output_label = gr.Textbox(visible=False)\n",
    "\n",
    "    file_input.change(process_file, inputs=file_input, outputs=output_label)\n",
    "\n",
    "    msg = gr.Textbox(label=\"Ask a question\")\n",
    "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
    "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1.0, value=0.2, step=0.1, info=\"Regulates the creativity of the answers\")\n",
    "        max_tokens = gr.Slider(label=\"Max tokens\", value=64, maximum=256, minimum=8, step=1, info=\"Regulates the length of the answers\")\n",
    "    completion = gr.Textbox(label=\"Response\")\n",
    "    btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    clear = gr.ClearButton(components=[msg, completion], value=\"Clear console\", variant=\"stop\")\n",
    "    \n",
    "    btn.click(respond, inputs=[msg, output_label, temperature, max_tokens], outputs=[completion])\n",
    "    msg.submit(respond, inputs=[msg, output_label, temperature, max_tokens], outputs=[completion])\n",
    "\n",
    "gr.close_all()\n",
    "demo.queue().launch(share=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7890\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
